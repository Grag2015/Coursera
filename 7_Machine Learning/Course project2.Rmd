---
title: "Machine learning knows how well you perform barbell lifts"
author: "Grag"
date: "31/01/2016"
---

```{r results='hide', echo=FALSE, warning=FALSE, message=FALSE}
library(randomForest)
library("ggplot2")
library("caret")
set.seed(1)
```

## Loading the data
```{r}
df = read.csv("pml-training.csv")
```
```{r echo=FALSE}
user_name <- df$user_name
```

## Preprocessing the data
```{r}
dim(df)
```
Our dataset has 160 variables. I'm going to reduce dimension of the data.

In first, I've removed some zero variance predictors
```{r}
df <- df[, -nearZeroVar(df)]

```

It's easy to check dataset has very many NA-values
```{r}
sum(sapply(df, function(e) sum(is.na(e))))
```

There are variables wich contain more than 80% NA-values 
```{r}
na80 <- sapply(df, function(e) sum(is.na(e)))/19622>0.8
names(df)[na80][1:10]
```

I've  removed such predictors
```{r}
df <- df[,!na80]
```

And I'll delete meta-information from list of predictors
```{r}
names(df[,1:6])
df <- df[,-c(1:6)]
```

In the end, I've removed some linear dependencies
```{r}
df <- df[, -findCorrelation(cor(df[,-53]), .95)]
```
So, now I have 49 variables 

Next, because test-set "pml-testing.csv" doesn't contain outcome, I'll create test set from
training data 
```{r}
set.seed(13)
inTrain <- createDataPartition(y=df$classe, p=0.75, list = F) 
training <- df[inTrain,]
testing <- df[-inTrain,]
```

## Principal component analysis
I decided reduce dimension using principal component analysis (PCA). It wasn't usefull
for fitting, but I found something intresting.
```{r}
preProc<-preProcess(training, method="pca", thresh = 0.99)
trainingPC <- predict(preProc, training[,-49]) 
preProc$numComp

```
So, we made 36 components wich captured 99% "information"

Now, I'll show what kind of "information"" we captured using principal components
```{r echo=F, warning=FALSE, message=FALSE}
ggplot(data = data.frame(trainingPC,user=user_name[inTrain]), aes(x=PC1, y=PC2, col=user))+
    geom_point()
```

What does it mean? I think, this picture says us, that in first there are strong pattern for every users, and in second 2 first prinipal components (wich you see on pict) contain information about
this pattern. But unfortunally information about users (who made excercises) is useless for us, 
because we are going to fit model, wich can work for everybody.
And I supposed that PCA dosn't help me. To check it I fitted 2 decision trees - with source data nd with PCA-preprocessing data. Results you can see in the table below.

```{r echo=F, cache=TRUE, warning=FALSE, message=FALSE}
fit_tree <- train(classe~., data = training, method="rpart")
pred1 <- predict(fit_tree, newdata=testing)
acc1 <- confusionMatrix(pred1, testing$classe)$overall[1]

fit_tree2 <- train(classe~., data = data.frame(trainingPC, classe=training$classe), method="rpart")
testingPC <- predict(preProc, newdata = testing[,-49])
pred2 <- predict(fit_tree2, newdata=testingPC)
acc2 <- confusionMatrix(pred2, testing$classe)$overall[1]

```


Accuracy without PCA  | Accuracy with PCA
------------- | -------------
`r acc1`  | `r acc2`


Next step I tryed some more popular LM models and compared results (testing accuracy)
(Here I didn't use meta-methods such as boosting, bagging and so on)

## Tree
This model I made above, when I was estimating PCA. Accuraccy was not very high 
```{r warning=FALSE, message=FALSE}
varImp(fit_tree)
```
here we see list of the most impotant variables. I would look for some pattern on scatter-plot
with top-variables
```{r echo=F, warning=FALSE, message=FALSE}
library(gridExtra)
g1 <- ggplot(data = training, aes(x=magnet_belt_y , y=total_accel_belt, col=classe, alpha=0.5))+
    geom_point()
g2 <- ggplot(data = training, aes(x=magnet_belt_y , y=yaw_belt, col=classe))+
    geom_point()

grid.arrange(g1, g2, ncol=1)
```
We can see some pattern on graphs, but in general classes mixed and it's difficult to obtain high accuracy use simple decision tree. Maybe we can improve result using boosting? 

## Linear discriminant analysis
Next I tested LDA, I didn't expect high level of accuracy, but I was surprised.
This model didn't take many time and I use cross-validation.
```{r cache=TRUE}
fitControl <- trainControl(method = "cv", number = 10, repeats = 10)
fit_lda <- train(classe~., data=training, method="lda", trControl=fitControl)

pred_fit_lda <- predict(fit_lda, newdata = testing)
acc3 <- confusionMatrix(pred_fit_lda, testing$classe)$overall[1]
```
So, we have quite good level of accuracy - `r acc3`

## LDA with bagging 
Next step, I'll try to improve result using bootstrap aggregating (bagging)
```{r cache=TRUE, warning=FALSE, message=FALSE}
# lda with bagging 
bagLDA2 <- train(training[,-49], training$classe, "bag", B = 10, 
                 bagControl = bagControl(fit = ldaBag$fit,
                                         predict = ldaBag$pred,
                                         aggregate = ldaBag$aggregate))
pred_fit_ldabag <- predict(bagLDA2, newdata = testing)
acc4 <- confusionMatrix(pred_fit_ldabag, testing$classe)$overall[1]
```
But bagging couldn't increase accuracy - `r acc4` 

## Blending = tree + LDA
Tree and LDA two different classifiers and we can use blending
```{r cache=TRUE, warning=FALSE, message=FALSE}
predDF<- data.frame(pred_fit_lda, pred1, classe=testing$classe)
fit_stack <- train(classe~., data=predDF, method="rf")

pred_fit_stack <- predict(fit_stack, newdata = testing)
acc6 <- confusionMatrix(pred_fit_stack, testing$classe)$overall[1]
```
SO: Blending didn't very well increase accuracy - `r acc6`

## Random forest
When we have more than 30 predictors it's reasonably to use random forest.
```{r cache=TRUE}
fit_rf3= randomForest(classe~., data=training, mtry=20, ntree=200)
pred_fit_rf3 <- predict(fit_rf3, newdata = testing)

acc5 <- confusionMatrix(pred_fit_rf3, testing$classe)$overall[1]
```
So, we have excelent result: accuracy - `r acc5`

**Resume:**

Model  | Accuracy
------------- | -------------
Tree  | `r acc1`
Tree+PCA  | `r acc2`
LDA  | `r acc3`
LDA bagging  | `r acc4`
Stack LDA+Tree  | `r acc6`
Random forest  | `r acc5`

```{r}
library(knitr)
getwd()
setwd()
knit2html("Course project2.Rmd")
```

